Expanding Explainability: Towards Social Transparency in AI systems
--
Ehsan et al. CHI 2021 Best Paper <BR>
Source: https://arxiv.org/pdf/2101.04719.pdf

Background
--
- As AI systems are rapidly being employed in high stakes decision-making scenarios in industries such as healthcare, finance, college admissions, hiring, and criminal justice, the need for explainability becomes paramount
- There has been commendable progress in XAI -- esp. around algorithmic approaches to generate representations of how a ML model operates or makes decisions 

Problem
--
- XAI techniques used in human-AI interactions are in fact ineffective, potentially risky, and underused
- Also, these techniques easily fall prey to Solutionism or Formalism; only technical view on explainability cannot address the human-AI assemblages of AI systems
- However, explanations of the AI system inherently lie within a socio-organizational context

Proposal
--
- Introduce and explore Social Transparency (ST) in AI systems

Social Transparency
--
- Extends the visibility of one's direct partner and the effect on their dyadic interactions, to also encompass one's role as an observer of others' interactions made visible in the network 
  - Stuart et al.:
    - identity transparency
    - content transparency
    - interaction transparency
- ST in AI systems has the following distinctions:
  - interested in making others' activities visible & how others' interactions with AI impact the explainability of the system 
  
Methods
--
- Conduct semi-structured interviews with 29 participants with visual scenarios 
![Screen Shot 2021-07-20 at 9 56 01 PM](https://user-images.githubusercontent.com/38929910/126327843-ba7b2afb-a508-4d95-a0ae-f41b4c5576b8.png)

Contributions
--
- Highlight an epistemic blind spot in XAI 
- Explore the concept of ST in AI systems and develop a scenario-based speculative design that embodies ST
- Conduct a formative study and empirically derive a conceptual framework 
- Share design insights and potential challenges 
